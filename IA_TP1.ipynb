{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17efab02-67f9-46e1-bcf0-784cfbcda726",
   "metadata": {},
   "source": [
    "# TP 1 - Pratique du NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b121bd-7a7c-42b8-8a04-74c66673a5bc",
   "metadata": {},
   "source": [
    "# Partie 1 — Préparation de l’environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0627522a-5050-4f78-9e73-4805c7f37a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliothèques importées avec succès!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ressources NLTK téléchargées!\n",
      "Composants importés avec succès!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importation de bibliothèques\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "import re\n",
    "import string\n",
    "#from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Bibliothèques importées avec succès!\")\n",
    "\n",
    "#Téléchargement de ressources NLTK\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"Ressources NLTK téléchargées!\")\n",
    "\n",
    "# Chargement des composants NLTK\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "print(\"Composants importés avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6885c-13c0-479d-b285-5c1be1189c4f",
   "metadata": {},
   "source": [
    "# Réponses aux questions :\n",
    "    \n",
    "## 1. Commande pip pour installer une bibliothèque :\n",
    "\n",
    "###  pip install nom_bibliotheque\n",
    "\n",
    "## 2. Différence entre nltk et spaCy :\n",
    "\n",
    "### NLTK : plus pédagogique et modulairee\n",
    "\n",
    "### spaCy : optimisé pour la production et l’analyse rapide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc64c5b-44ee-4062-8f81-d605d09f7f3e",
   "metadata": {},
   "source": [
    "# Partie 2 — Nettoyage du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8031a6e5-d6db-4a67-935f-dc8f79198993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTE ORIGINAL:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLe Natural Language Processing (NLP) est véritablement FASCINANT ! \\nIl permet aux machines de COMPRENDRE le langage humain complexe.\\nLes modèles comme BERT, GPT-4, etc., révolutionnent l'Intelligence Artificielle.\\nN'oublions pas les accents : été, hôtel, naïve. C'est incroyable !\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Le Natural Language Processing (NLP) est véritablement FASCINANT ! \n",
    "Il permet aux machines de COMPRENDRE le langage humain complexe.\n",
    "Les modèles comme BERT, GPT-4, etc., révolutionnent l'Intelligence Artificielle.\n",
    "N'oublions pas les accents : été, hôtel, naïve. C'est incroyable !\n",
    "\"\"\"\n",
    "print(\"TEXTE ORIGINAL:\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e0dfba-c2dc-4687-9bc1-824ecb218570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTE EN MINISCULES:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nle natural language processing (nlp) est véritablement fascinant ! \\nil permet aux machines de comprendre le langage humain complexe.\\nles modèles comme bert, gpt-4, etc., révolutionnent l'intelligence artificielle.\\nn'oublions pas les accents : été, hôtel, naïve. c'est incroyable !\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_min = text.lower()\n",
    "print(\"TEXTE EN MINISCULES:\")\n",
    "text_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95a8609-9299-4a30-9ee8-9952354210ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTE NETTOYE:\n",
      "le natural language processing nlp est véritablement fascinant il permet aux machines de comprendre le langage humain complexe les modèles comme bert gpt 4 etc révolutionnent l intelligence artificielle n oublions pas les accents été hôtel naïve c est incroyable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suppression de la ponctuation\n",
    "def nettoyer_texte(texte):\n",
    "    # Supprimer la ponctuation\n",
    "    texte = re.sub(r'[^\\w\\s]', ' ', texte)\n",
    "    # Supprimer les espaces multiples\n",
    "    texte = re.sub(r'\\s+', ' ', texte)\n",
    "    return texte.strip()\n",
    "\n",
    "text_propre = nettoyer_texte(text_min)\n",
    "print(\"TEXTE NETTOYE:\")\n",
    "print(text_propre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143787d-ad11-4d6f-9938-8ffb099c8e6a",
   "metadata": {},
   "source": [
    "# Réponses aux questions :\n",
    "    \n",
    "## Pourquoi est-il important de mettre le texte en minuscules ?\n",
    "\n",
    "* Évite la duplication (ex: 'Chat' et 'chat')\n",
    "* Uniformise le texte pour l'analyse\n",
    "* Améliore la performance des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25778-6e88-4741-a8a9-9fa3cd52bb1c",
   "metadata": {},
   "source": [
    "# Partie 3 — Tokenisation et Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55369042-a0b8-4d13-9a9f-8a3955e7b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'véritablement',\n",
       " 'fascinant',\n",
       " 'permet',\n",
       " 'machines',\n",
       " 'comprendre',\n",
       " 'langage',\n",
       " 'humain',\n",
       " 'complexe',\n",
       " 'modèles',\n",
       " 'comme',\n",
       " 'bert',\n",
       " 'gpt',\n",
       " '4',\n",
       " 'etc',\n",
       " 'révolutionnent',\n",
       " 'intelligence',\n",
       " 'artificielle',\n",
       " 'oublions',\n",
       " 'accents',\n",
       " 'hôtel',\n",
       " 'naïve',\n",
       " 'incroyable']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenisation avec NLTK\n",
    "tokens = word_tokenize(text_propre)\n",
    "\n",
    "# Récupération des stopwords français\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Filtrage des stopwords\n",
    "tokens_filtres = [w for w in tokens if w not in stop_words]\n",
    "tokens_filtres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae61f9ed-b487-4100-ae15-3f544f10d57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'véritablement',\n",
       " 'fascinant',\n",
       " 'permet',\n",
       " 'machines',\n",
       " 'comprendre',\n",
       " 'langage',\n",
       " 'humain',\n",
       " 'complexe',\n",
       " 'modèles',\n",
       " 'bert',\n",
       " 'gpt',\n",
       " '4',\n",
       " 'etc',\n",
       " 'révolutionnent',\n",
       " 'intelligence',\n",
       " 'artificielle',\n",
       " 'oublions',\n",
       " 'accents',\n",
       " 'hôtel',\n",
       " 'naïve',\n",
       " 'incroyable']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajout d'un mot personnalisé aux stopwords\n",
    "stop_words.add('comme')\n",
    "tokens_filtres = [w for w in tokens if w not in stop_words]\n",
    "tokens_filtres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c51d4c-9a29-4d54-9a40-de85b8f30219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSE QUANTITATIVE:\n",
      "Nombre de mots originaux: 39\n",
      "Nombre de mots après filtrage: 25\n",
      "Proportion supprimée: 35.9%\n"
     ]
    }
   ],
   "source": [
    "# Analyse quantitative\n",
    "mots_alpha = [token for token in tokens if token.isalpha()]\n",
    "pourcentage_supprime = (len(mots_alpha) - len(tokens_filtres)) / len(mots_alpha) * 100\n",
    "\n",
    "print(\"ANALYSE QUANTITATIVE:\")\n",
    "print(f\"Nombre de mots originaux: {len(mots_alpha)}\")\n",
    "print(f\"Nombre de mots après filtrage: {len(tokens_filtres)}\")\n",
    "print(f\"Proportion supprimée: {pourcentage_supprime:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f3452-9e2f-4d2f-865f-435e3407165d",
   "metadata": {},
   "source": [
    "# Réponses aux questions :\n",
    "    \n",
    "## Que représentent les stopwords ?\n",
    "\n",
    "Mots très fréquents mais peu informatifs\n",
    "\n",
    "## Quelle proportion du texte original est supprimée après leur retrait ?\n",
    "\n",
    "La proportion supprimée est: 35.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443abf0-0fbd-4e8b-a5d3-bd3cd0bc5862",
   "metadata": {},
   "source": [
    "# Partie 4 — Lemmatisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b54473a-082e-43f8-897e-b3bf9144689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'languag',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'véritablement',\n",
       " 'fasciner',\n",
       " 'permettre',\n",
       " 'machine',\n",
       " 'comprendre',\n",
       " 'langage',\n",
       " 'humain',\n",
       " 'complexe',\n",
       " 'modèle',\n",
       " 'bert',\n",
       " 'gpt',\n",
       " '4',\n",
       " 'etc',\n",
       " 'révolutionner',\n",
       " 'intelligence',\n",
       " 'artificiel',\n",
       " 'oublion',\n",
       " 'accent',\n",
       " 'hôtel',\n",
       " 'naïf',\n",
       " 'incroyable']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(\" \".join(tokens_filtres))\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae8552-39c0-405e-ad86-987d430f045b",
   "metadata": {},
   "source": [
    "# Réponses aux questions :\n",
    "    \n",
    "## Quelle est la différence entre stemming et lemmatisation ?\n",
    "\n",
    "* Stemming: réduction radicale ('running' → 'run', 'better' → 'bet')\n",
    "* Lemmatisation: forme canonique ('running' → 'run', 'better' → 'good')\n",
    "\n",
    "## Pourquoi cette étape est-elle importante avant l’analyse sémantique ?\n",
    "\n",
    "* Regroupe les formes variées d'un même concept\n",
    "* Réduit la dimensionnalité des données\n",
    "* Améliore la représentation sémantique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
